{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"aitextgen \u00b6 A robust tool for advanced AI text generation via GPT-2 . aitextgen is a Python package that leverages PyTorch , Huggingface Transformers and pytorch-lightning with specific optimizations for text generation using GPT-2, plus many added features. It is the successor to textgenrnn and gpt-2-simple , taking the best of both packages: Finetunes on a pretrained 124M GPT-2 model from OpenAI...or create your own GPT-2 model + tokenizer and train from scratch! Generates text faster than gpt-2-simple and with better memory efficiency! (even from the 1.5B GPT-2 model !) With Transformers, aitextgen preserves compatibility with the base package, allowing you to use the model for other NLP tasks, download custom GPT-2 models from the Huggingface model repository, and upload your own models! Also, it uses the included generate() function to allow a massive amount of control over the generated text. With pytorch-lightning, aitextgen trains models not just on CPUs and GPUs, but also multiple GPUs and (eventually) TPUs! It also includes a pretty training progress bar, with the ability to add optional loggers. The input dataset is its own object, allowing you to not only easily encode megabytes of data in seconds, cache, and compress it on a local computer before transporting to a remote server, but you are able to merge datasets without biasing the resulting dataset, or cross-train on multiple datasets to create blended output. Installation \u00b6 aitextgen can be installed from PyPI: pip3 install aitextgen More \u00b6 For more info on how to use aitextgen, check out the nav sidebar, or you can do a Hello World tutorial .","title":"Home"},{"location":"#aitextgen","text":"A robust tool for advanced AI text generation via GPT-2 . aitextgen is a Python package that leverages PyTorch , Huggingface Transformers and pytorch-lightning with specific optimizations for text generation using GPT-2, plus many added features. It is the successor to textgenrnn and gpt-2-simple , taking the best of both packages: Finetunes on a pretrained 124M GPT-2 model from OpenAI...or create your own GPT-2 model + tokenizer and train from scratch! Generates text faster than gpt-2-simple and with better memory efficiency! (even from the 1.5B GPT-2 model !) With Transformers, aitextgen preserves compatibility with the base package, allowing you to use the model for other NLP tasks, download custom GPT-2 models from the Huggingface model repository, and upload your own models! Also, it uses the included generate() function to allow a massive amount of control over the generated text. With pytorch-lightning, aitextgen trains models not just on CPUs and GPUs, but also multiple GPUs and (eventually) TPUs! It also includes a pretty training progress bar, with the ability to add optional loggers. The input dataset is its own object, allowing you to not only easily encode megabytes of data in seconds, cache, and compress it on a local computer before transporting to a remote server, but you are able to merge datasets without biasing the resulting dataset, or cross-train on multiple datasets to create blended output.","title":"aitextgen"},{"location":"#installation","text":"aitextgen can be installed from PyPI: pip3 install aitextgen","title":"Installation"},{"location":"#more","text":"For more info on how to use aitextgen, check out the nav sidebar, or you can do a Hello World tutorial .","title":"More"},{"location":"cli/","text":"Command-Line Interface \u00b6 aitextgen has a command-line interface to quickly automate common tasks, and make it less necessary to use a script; helpful if running on a remote server. Encode \u00b6 Encodes given text text.txt into a cache and compressed TokenDataset , good for prepping a dataset for transit to a remote server. aitextgen encode text.txt If you are encoding a CSV, you should pass in the line_by_line parameter as well. aitextgen encode reddit.csv --line_by_line True Train \u00b6 To train/finetune on the default 124M GPT-2, given text text.txt and all default parameters: aitextgen train text.txt If you are using a cached/compressed dataset that ends with tar.gz (e.g one created by the Encoding CLI command above), you can pass that to this function as well. aitextgen train dataset_cache.tar.gz Other parameters to the TokenDataset constructor can be used. Generate \u00b6 Loads a model and generates to a file. By default, it will generate 20 texts to the file, 1 at a time at temperature of 0.7. aitextgen generate You can print to console instead by passing --to_file False aitextgen generate --prompt \"I believe in unicorns because\" --to_file False","title":"Command-Line Interface"},{"location":"cli/#command-line-interface","text":"aitextgen has a command-line interface to quickly automate common tasks, and make it less necessary to use a script; helpful if running on a remote server.","title":"Command-Line Interface"},{"location":"cli/#encode","text":"Encodes given text text.txt into a cache and compressed TokenDataset , good for prepping a dataset for transit to a remote server. aitextgen encode text.txt If you are encoding a CSV, you should pass in the line_by_line parameter as well. aitextgen encode reddit.csv --line_by_line True","title":"Encode"},{"location":"cli/#train","text":"To train/finetune on the default 124M GPT-2, given text text.txt and all default parameters: aitextgen train text.txt If you are using a cached/compressed dataset that ends with tar.gz (e.g one created by the Encoding CLI command above), you can pass that to this function as well. aitextgen train dataset_cache.tar.gz Other parameters to the TokenDataset constructor can be used.","title":"Train"},{"location":"cli/#generate","text":"Loads a model and generates to a file. By default, it will generate 20 texts to the file, 1 at a time at temperature of 0.7. aitextgen generate You can print to console instead by passing --to_file False aitextgen generate --prompt \"I believe in unicorns because\" --to_file False","title":"Generate"},{"location":"dataset/","text":"TokenDataset \u00b6 aitextgen has a special class, TokenDataset , used for managing tokenized datasets to be fed into model training. (this is in contrast with other GPT-2 finetuning approaches, which tokenizes at training time although you can still do that if you want) This has a few nice bonuses, including: Tokenize a dataset on a local machine ahead of time and compress it, saving time/bandwidth transporting data to a remote machine Supports both reading a dataset line-by-line (including single-column CSVs), or bulk texts. Merge datasets together without using external libraries Cross-train on multiple datasets to \"blend\" them together. Creating a TokenDataset For GPT-2 Finetuning \u00b6 The easiest way to create a TokenDataset is to provide a target file. If no vocab_file and merges_file are provided, it will use the default GPT-2 tokenizer. from aitextgen.TokenDataset import TokenDataset data = TokenDataset ( \"shakespeare.txt\" ) If you pass a single-column CSV and specify line_by_line=True , the TokenDataset will parse it row-by-row, and is the recommended way to handle multiline texts. data = TokenDataset ( \"politics.csv\" , line_by_line = True ) You can also manually pass a list of texts to texts instead if you've processed them elsewhere. data = TokenDataset ( texts = [ \"Lorem\" , \"Ipsum\" , \"Dolor\" ]) Saving/Loading a TokenDataset \u00b6 When creating a TokenDataset, you can automatically save it as a compressed gzipped MessagePack binary when completed. data = TokenDataset ( \"shakespeare.txt\" , save_cache = True ) Or save it after you've loaded it with the save() function. data = TokenDataset ( \"shakespeare.txt\" ) data . save () By default, it will save to dataset_cache.tar.gz . You can then reload that into another Python session by specifying the cache. data = TokenDataset ( \"dataset_cache.tar.gz\" , from_cache = True ) Using TokenDatasets with a Custom GPT-2 Model \u00b6 The default TokenDataset has a block_size of 1024 , which corresponds to the context window of the default GPT-2 model . If you're using a custom model w/ a different maximum. Additionally, you must explicitly provide the vocab and merges files to rebuild the tokenizer, as the tokenizer will be different than the normal GPT-2 one. See the Model From Scratch docs for more info. Merging TokenDatasets \u00b6 Merging processed TokenDatasets can be done with the merge_datasets() function. By default, it will take samples equal to the smallest dataset from each TokenDataset, randomly sampling the appropriate number of texts from the larger datasets. This will ensure that model training does not bias toward one particular dataset. (it can be disabled by setting equalize=False ) (You can merge bulk datasets and line-by-line datasets, but the training output may be bizarre!) About Merging The current implementation merges by subset count, so equalization may not be perfect. from aitextgen.TokenDataset import TokenDataset , merge_datasets data1 = TokenDataset ( \"politics1000.csv\" , line_by_line = True ) # 1000 samples data2 = TokenDataset ( \"politics4000.csv\" , line_by_line = True ) # 4000 samples data_merged = merge_datasets ([ data1 , data2 ]) # ~2000 samples","title":"TokenDataset"},{"location":"dataset/#tokendataset","text":"aitextgen has a special class, TokenDataset , used for managing tokenized datasets to be fed into model training. (this is in contrast with other GPT-2 finetuning approaches, which tokenizes at training time although you can still do that if you want) This has a few nice bonuses, including: Tokenize a dataset on a local machine ahead of time and compress it, saving time/bandwidth transporting data to a remote machine Supports both reading a dataset line-by-line (including single-column CSVs), or bulk texts. Merge datasets together without using external libraries Cross-train on multiple datasets to \"blend\" them together.","title":"TokenDataset"},{"location":"dataset/#creating-a-tokendataset-for-gpt-2-finetuning","text":"The easiest way to create a TokenDataset is to provide a target file. If no vocab_file and merges_file are provided, it will use the default GPT-2 tokenizer. from aitextgen.TokenDataset import TokenDataset data = TokenDataset ( \"shakespeare.txt\" ) If you pass a single-column CSV and specify line_by_line=True , the TokenDataset will parse it row-by-row, and is the recommended way to handle multiline texts. data = TokenDataset ( \"politics.csv\" , line_by_line = True ) You can also manually pass a list of texts to texts instead if you've processed them elsewhere. data = TokenDataset ( texts = [ \"Lorem\" , \"Ipsum\" , \"Dolor\" ])","title":"Creating a TokenDataset For GPT-2 Finetuning"},{"location":"dataset/#savingloading-a-tokendataset","text":"When creating a TokenDataset, you can automatically save it as a compressed gzipped MessagePack binary when completed. data = TokenDataset ( \"shakespeare.txt\" , save_cache = True ) Or save it after you've loaded it with the save() function. data = TokenDataset ( \"shakespeare.txt\" ) data . save () By default, it will save to dataset_cache.tar.gz . You can then reload that into another Python session by specifying the cache. data = TokenDataset ( \"dataset_cache.tar.gz\" , from_cache = True )","title":"Saving/Loading a TokenDataset"},{"location":"dataset/#using-tokendatasets-with-a-custom-gpt-2-model","text":"The default TokenDataset has a block_size of 1024 , which corresponds to the context window of the default GPT-2 model . If you're using a custom model w/ a different maximum. Additionally, you must explicitly provide the vocab and merges files to rebuild the tokenizer, as the tokenizer will be different than the normal GPT-2 one. See the Model From Scratch docs for more info.","title":"Using TokenDatasets with a Custom GPT-2 Model"},{"location":"dataset/#merging-tokendatasets","text":"Merging processed TokenDatasets can be done with the merge_datasets() function. By default, it will take samples equal to the smallest dataset from each TokenDataset, randomly sampling the appropriate number of texts from the larger datasets. This will ensure that model training does not bias toward one particular dataset. (it can be disabled by setting equalize=False ) (You can merge bulk datasets and line-by-line datasets, but the training output may be bizarre!) About Merging The current implementation merges by subset count, so equalization may not be perfect. from aitextgen.TokenDataset import TokenDataset , merge_datasets data1 = TokenDataset ( \"politics1000.csv\" , line_by_line = True ) # 1000 samples data2 = TokenDataset ( \"politics4000.csv\" , line_by_line = True ) # 4000 samples data_merged = merge_datasets ([ data1 , data2 ]) # ~2000 samples","title":"Merging TokenDatasets"},{"location":"ethics/","text":"Ethics \u00b6 aitextgen is a tool primarily intended to help facilitate creative content. It is not a tool intended to deceive. Although parody accounts are an obvious use case for this package, make sure you are as upfront as possible with the methodology of the text you create. This includes: State that the text was generated using aitextgen and/or a GPT-2 model architecture. (a link to this repo would be a bonus!) If parodying a person, explicitly state that it is a parody, and reference who it is parodying. If the generated human-curated, or if it's unsupervised random output Indicating who is maintaining/curating the AI-generated text. Make a good-faith effort to remove overfit output from the generated text that matches the input text verbatim. It's fun to anthropomorphise the nameless \"AI\" as an abstract genius, but part of the reason I made aitextgen (and all my previous text-generation projects) is to make the technology more accessible and accurately demonstrate both its promise, and its limitations. Any AI text generation projects that are deliberately deceptive may be disavowed.","title":"Ethics"},{"location":"ethics/#ethics","text":"aitextgen is a tool primarily intended to help facilitate creative content. It is not a tool intended to deceive. Although parody accounts are an obvious use case for this package, make sure you are as upfront as possible with the methodology of the text you create. This includes: State that the text was generated using aitextgen and/or a GPT-2 model architecture. (a link to this repo would be a bonus!) If parodying a person, explicitly state that it is a parody, and reference who it is parodying. If the generated human-curated, or if it's unsupervised random output Indicating who is maintaining/curating the AI-generated text. Make a good-faith effort to remove overfit output from the generated text that matches the input text verbatim. It's fun to anthropomorphise the nameless \"AI\" as an abstract genius, but part of the reason I made aitextgen (and all my previous text-generation projects) is to make the technology more accessible and accurately demonstrate both its promise, and its limitations. Any AI text generation projects that are deliberately deceptive may be disavowed.","title":"Ethics"},{"location":"generate-performance/","text":"Improving Generation Performance \u00b6 A few tips and tricks for improving generation performance for both on CPUs and GPUs. (note that with these tricks, you cannot train the model afterwards!) CPU \u00b6 Quantization \u00b6 PyTorch has the ability to quantize models on the CPU. Currently, it will only quantize the Linear layer of GPT-2, but the generation performances increases 15% \u2014 25% ; far from trivial! To quantize a model after it's loaded, just run: ai . quantize () GPU \u00b6 FP16 \u00b6 Certain GPUs, notably the cheap T4 and the expensive V100, support the ability to process models using FP16, giving massive speed and memory improvements, Assuming you are using a compatable GPU and already have apex installed, you can convert a model to the \"half\" FP16 mode with this: ai . to_fp16 () If you want to convert the model before loading it into GPU memory (which may help avoid memory leaks), you can instantiate the model like this: ai . to_fp16 ( to_gpu = True , to_fp16 = True ) With this, you can generate massive amounts of text from even the GPT-2 1.5B model!","title":"Improving Generation Performance"},{"location":"generate-performance/#improving-generation-performance","text":"A few tips and tricks for improving generation performance for both on CPUs and GPUs. (note that with these tricks, you cannot train the model afterwards!)","title":"Improving Generation Performance"},{"location":"generate-performance/#cpu","text":"","title":"CPU"},{"location":"generate-performance/#quantization","text":"PyTorch has the ability to quantize models on the CPU. Currently, it will only quantize the Linear layer of GPT-2, but the generation performances increases 15% \u2014 25% ; far from trivial! To quantize a model after it's loaded, just run: ai . quantize ()","title":"Quantization"},{"location":"generate-performance/#gpu","text":"","title":"GPU"},{"location":"generate-performance/#fp16","text":"Certain GPUs, notably the cheap T4 and the expensive V100, support the ability to process models using FP16, giving massive speed and memory improvements, Assuming you are using a compatable GPU and already have apex installed, you can convert a model to the \"half\" FP16 mode with this: ai . to_fp16 () If you want to convert the model before loading it into GPU memory (which may help avoid memory leaks), you can instantiate the model like this: ai . to_fp16 ( to_gpu = True , to_fp16 = True ) With this, you can generate massive amounts of text from even the GPT-2 1.5B model!","title":"FP16"},{"location":"generate/","text":"Text Generation \u00b6 Thanks to the base Transformers package, aitextgen has more options for generating text than other text-generating apps before. Generation Parameters \u00b6 See this article by Huggingface engineer Patrick von Platen for how sampling and these parameters are used in practice. n : Number of texts generated. max_length : Maximum length of the generated text (default: 200; for GPT-2, the maximum is 1024.) prompt : Prompt that starts the generated text and is included in the generate text. (used to be prefix in previous tools) temperature : Controls the \"craziness\" of the text (default: 0.7) top_k : If nonzero, limits the sampled tokens to the top k values. (default: 0) top_p : If nonzero, limits the sampled tokens to the cumulative probability Some lesser-known-but-still-useful-parameters that are unique to Transformers: num_beams : If greater than 1, executes beam search for cleaner text. repetition_penalty : If greater than 1.0, penalizes repetition in a text to avoid infinite loops. length_penalty : If greater than 1.0, penalizes text proportional to the length no_repeat_ngram_size : Token length to avoid repeating given phrases. Generation Functions \u00b6 Given a aitextgen object with a loaded model + tokenizer named ai : About devices aitextgen does not automatically set the device used to generate text. If you want to generate on the GPU, make sure you call ai.to_gpu() beforehand, or load the model into the GPU using ai = aitextgen(to_gpu=True) ai.generate() : Generates and prints text to console. If prompt is used, the prompt is bolded. (a la Talk to Transformer ) ai.generate_one() : A helper function which generates a single text and returns as a string (good for APIs) ai.generate_samples() : Generates multiple samples at specified temperatures: great for debugging. ai.generate_to_file() : Generates a bulk amount of texts to file. (this accepts a batch_size parameter which is useful if using on a GPU) Cleanup By default, the cleanup parameter is set to True, which automatically removes texts that are blatantly malformed (e.g. only 2 characters long). Therefore, there may be less than n results returned. You can disabled this behavior by setting cleanup=False . Seed \u00b6 aitextgen has a new seed parameter for generation. Using any generate function with a seed parameter (must be an integer) and all other models/parameters the same, and the generated text will be identical. This allows for reproducible generations. For generate_to_file() , the 8-digit number at the end of the file name will be the seed used to generate the file, making reprodicibility easy.","title":"Generating Text"},{"location":"generate/#text-generation","text":"Thanks to the base Transformers package, aitextgen has more options for generating text than other text-generating apps before.","title":"Text Generation"},{"location":"generate/#generation-parameters","text":"See this article by Huggingface engineer Patrick von Platen for how sampling and these parameters are used in practice. n : Number of texts generated. max_length : Maximum length of the generated text (default: 200; for GPT-2, the maximum is 1024.) prompt : Prompt that starts the generated text and is included in the generate text. (used to be prefix in previous tools) temperature : Controls the \"craziness\" of the text (default: 0.7) top_k : If nonzero, limits the sampled tokens to the top k values. (default: 0) top_p : If nonzero, limits the sampled tokens to the cumulative probability Some lesser-known-but-still-useful-parameters that are unique to Transformers: num_beams : If greater than 1, executes beam search for cleaner text. repetition_penalty : If greater than 1.0, penalizes repetition in a text to avoid infinite loops. length_penalty : If greater than 1.0, penalizes text proportional to the length no_repeat_ngram_size : Token length to avoid repeating given phrases.","title":"Generation Parameters"},{"location":"generate/#generation-functions","text":"Given a aitextgen object with a loaded model + tokenizer named ai : About devices aitextgen does not automatically set the device used to generate text. If you want to generate on the GPU, make sure you call ai.to_gpu() beforehand, or load the model into the GPU using ai = aitextgen(to_gpu=True) ai.generate() : Generates and prints text to console. If prompt is used, the prompt is bolded. (a la Talk to Transformer ) ai.generate_one() : A helper function which generates a single text and returns as a string (good for APIs) ai.generate_samples() : Generates multiple samples at specified temperatures: great for debugging. ai.generate_to_file() : Generates a bulk amount of texts to file. (this accepts a batch_size parameter which is useful if using on a GPU) Cleanup By default, the cleanup parameter is set to True, which automatically removes texts that are blatantly malformed (e.g. only 2 characters long). Therefore, there may be less than n results returned. You can disabled this behavior by setting cleanup=False .","title":"Generation Functions"},{"location":"generate/#seed","text":"aitextgen has a new seed parameter for generation. Using any generate function with a seed parameter (must be an integer) and all other models/parameters the same, and the generated text will be identical. This allows for reproducible generations. For generate_to_file() , the 8-digit number at the end of the file name will be the seed used to generate the file, making reprodicibility easy.","title":"Seed"},{"location":"gpt-2-simple/","text":"Importing from gpt-2-simple \u00b6 Want to import a model trained using gpt-2-simple , or another GPT-2 based finetuning approach? You can do that using the transformers-cli . In the case of gpt-2-simple (where the output is structured checkpoint/run1 ), you'd cd into the directory containing the checkpoint folder and run: transformers-cli convert --model_type gpt2 --tf_checkpoint checkpoint/run1 --pytorch_dump_output pytorch --config checkpoint/run1/hparams.json This will put a pytorch_model.bin and config.json in the pytorch folder, which is what you'll need to pass to aitextgen() to load the model. That's it!","title":"Importing from gpt-2-simple"},{"location":"gpt-2-simple/#importing-from-gpt-2-simple","text":"Want to import a model trained using gpt-2-simple , or another GPT-2 based finetuning approach? You can do that using the transformers-cli . In the case of gpt-2-simple (where the output is structured checkpoint/run1 ), you'd cd into the directory containing the checkpoint folder and run: transformers-cli convert --model_type gpt2 --tf_checkpoint checkpoint/run1 --pytorch_dump_output pytorch --config checkpoint/run1/hparams.json This will put a pytorch_model.bin and config.json in the pytorch folder, which is what you'll need to pass to aitextgen() to load the model. That's it!","title":"Importing from gpt-2-simple"},{"location":"helpful-notes/","text":"Helpful Notes \u00b6 A few helpful tips and tricks for using aitextgen. Not all AI generated text will be good , hence why human curation is currently a necessary strategy for many finetuned models. In testing, only 5% \u2014 10% of generated text is viable. One of the design goals of aitextgen is to help provide tools to improve that signal-to-noise ratio. You may not necessarily get better results with larger models . Larger models perform better on academic benchmarks, yes, but the quality of text can vary strongly depending on the size of the model used, especially if you do not have a lot of input data. To convert a GPT-2 model trained using earlier TensorFlow-based finetuning tools such as gpt-2-simple to the PyTorch format, use the transformers-cli command and the instructions here to convert the checkpoint (where OPENAI_GPT2_CHECKPOINT_PATH is the folder containing the model) When running on Google Cloud Platform (including Google Colab), it's recommended to download the TF-based GPT-2 from the Google API vs. downloading the PyTorch GPT-2 from Huggingface as the download will be much faster and also saves Huggingface some bandwidth. If you want to generate text from a GPU, you must manually move the model to the GPU (it will not be done automatically to save GPU VRAM for training). Either call to_gpu=True when loading the model or call to_gpu() from the aitextgen object. Encoding your text dataset before moving it to a cloud/remote server is strongly recommended. You can do that quickly from the CLI ( aitextgen encode text.txt ) Thanks to a few tricks, the file size is reduced by about 1/2 to 2/3, and the encoded text will instantly load on the remote server! If you're making a micro-GPT-2 model, using a GPU with a large batch size is recommended, as with the AdamW optimizer, it effectively has built-in batch normalization. If you make frequent use of the Colab Notebooks, I recommend purchasing Colab Pro : the timeouts are infrequent, and being able to reliably train on a P100 (normally $1.46/hr ) for hours on end very quickly pays for itself!","title":"Helpful Notes"},{"location":"helpful-notes/#helpful-notes","text":"A few helpful tips and tricks for using aitextgen. Not all AI generated text will be good , hence why human curation is currently a necessary strategy for many finetuned models. In testing, only 5% \u2014 10% of generated text is viable. One of the design goals of aitextgen is to help provide tools to improve that signal-to-noise ratio. You may not necessarily get better results with larger models . Larger models perform better on academic benchmarks, yes, but the quality of text can vary strongly depending on the size of the model used, especially if you do not have a lot of input data. To convert a GPT-2 model trained using earlier TensorFlow-based finetuning tools such as gpt-2-simple to the PyTorch format, use the transformers-cli command and the instructions here to convert the checkpoint (where OPENAI_GPT2_CHECKPOINT_PATH is the folder containing the model) When running on Google Cloud Platform (including Google Colab), it's recommended to download the TF-based GPT-2 from the Google API vs. downloading the PyTorch GPT-2 from Huggingface as the download will be much faster and also saves Huggingface some bandwidth. If you want to generate text from a GPU, you must manually move the model to the GPU (it will not be done automatically to save GPU VRAM for training). Either call to_gpu=True when loading the model or call to_gpu() from the aitextgen object. Encoding your text dataset before moving it to a cloud/remote server is strongly recommended. You can do that quickly from the CLI ( aitextgen encode text.txt ) Thanks to a few tricks, the file size is reduced by about 1/2 to 2/3, and the encoded text will instantly load on the remote server! If you're making a micro-GPT-2 model, using a GPU with a large batch size is recommended, as with the AdamW optimizer, it effectively has built-in batch normalization. If you make frequent use of the Colab Notebooks, I recommend purchasing Colab Pro : the timeouts are infrequent, and being able to reliably train on a P100 (normally $1.46/hr ) for hours on end very quickly pays for itself!","title":"Helpful Notes"},{"location":"load-model/","text":"Model Loading \u00b6 There are several ways to load models. Loading an aitextgen model \u00b6 The closer to the default 124M GPT-2 model, the fewer files you need! For the base case, loading the default model via Huggingface: ai = aitextgen () The downloaded model will be downloaded to cache_dir : /aitextgen by default. If you've finetuned a 124M GPT-2 model using aitextgen, you can pass the generated pytorch_model.bin to aitextgen: ai = aitextgen ( model = \"pytorch_model.bin\" ) If you're loading a finetuned model of a different GPT-2 architecture, you'll must also pass the generated config.json to aitextgen: ai = aitextgen ( model = \"pytorch_model.bin\" , config = config ) If you want to download an alternative GPT-2 model from Huggingface's repository of models, pass that model name to model . ai = aitextgen ( model = \"minimaxir/hacker-news\" ) The model and associated config + tokenizer will be downloaded into cache_dir . Loading TensorFlow-based GPT-2 models \u00b6 aitextgen lets you download the models from Google's servers that OpenAI had uploaded back when GPT-2 was first released in 2019. These models are then converted to a PyTorch format. It's counterintuitive, but it's substantially faster than downloading from Huggingface's servers, especially if you are running your code on Google Cloud Platform (e.g. Colab notebooks) To use this workflow, pass the corresponding model number to tf_gpt2 : ai = aitextgen ( tf_gpt2 = \"124M\" ) This will cache the converted model locally in cache_dir , and using the same parameters will load the converted model. The valid TF model names are [\"124M\",\"355M\",\"774M\",\"1558M\"] .","title":"Loading a Model"},{"location":"load-model/#model-loading","text":"There are several ways to load models.","title":"Model Loading"},{"location":"load-model/#loading-an-aitextgen-model","text":"The closer to the default 124M GPT-2 model, the fewer files you need! For the base case, loading the default model via Huggingface: ai = aitextgen () The downloaded model will be downloaded to cache_dir : /aitextgen by default. If you've finetuned a 124M GPT-2 model using aitextgen, you can pass the generated pytorch_model.bin to aitextgen: ai = aitextgen ( model = \"pytorch_model.bin\" ) If you're loading a finetuned model of a different GPT-2 architecture, you'll must also pass the generated config.json to aitextgen: ai = aitextgen ( model = \"pytorch_model.bin\" , config = config ) If you want to download an alternative GPT-2 model from Huggingface's repository of models, pass that model name to model . ai = aitextgen ( model = \"minimaxir/hacker-news\" ) The model and associated config + tokenizer will be downloaded into cache_dir .","title":"Loading an aitextgen model"},{"location":"load-model/#loading-tensorflow-based-gpt-2-models","text":"aitextgen lets you download the models from Google's servers that OpenAI had uploaded back when GPT-2 was first released in 2019. These models are then converted to a PyTorch format. It's counterintuitive, but it's substantially faster than downloading from Huggingface's servers, especially if you are running your code on Google Cloud Platform (e.g. Colab notebooks) To use this workflow, pass the corresponding model number to tf_gpt2 : ai = aitextgen ( tf_gpt2 = \"124M\" ) This will cache the converted model locally in cache_dir , and using the same parameters will load the converted model. The valid TF model names are [\"124M\",\"355M\",\"774M\",\"1558M\"] .","title":"Loading TensorFlow-based GPT-2 models"},{"location":"loggers/","text":"Loggers \u00b6 You can create loggers with popular tools such as TensorBoard and Weights and Biases by leveraging pytorch-lightning's logger functionality. See their documentation on all the available options for loggers. For example, if you want to create a TensorBoard logger, you can create it: from pytorch_lightning import loggers tb_logger = loggers . TensorBoardLogger ( 'logs/' ) Then pass it to the loggers parameter for ai.train() . ai . train ( train_data = data , loggers = tb_logger )","title":"Loggers"},{"location":"loggers/#loggers","text":"You can create loggers with popular tools such as TensorBoard and Weights and Biases by leveraging pytorch-lightning's logger functionality. See their documentation on all the available options for loggers. For example, if you want to create a TensorBoard logger, you can create it: from pytorch_lightning import loggers tb_logger = loggers . TensorBoardLogger ( 'logs/' ) Then pass it to the loggers parameter for ai.train() . ai . train ( train_data = data , loggers = tb_logger )","title":"Loggers"},{"location":"save-model/","text":"Model Saving \u00b6 There are are multiple ways to save models. Whenever a model is saved, two files are generated: pytorch_model.bin which contains the model weights, and config.json which is needed to load the model if it is not the base 124M GPT-2. Assuming we have an aitextgen model ai : Ad Hoc saving \u00b6 The aitextgen model can be saved at any time using save . ai . save () Save to Google Drive \u00b6 If you are using Google Colaboratory, you can mount your personal Google Drive to the notebook and save your models there. Downloading models from Colab Notebooks It's strongly recommended to move models to Google Drive before downloading them from Colaboratory. First mount your Google Drive using mount_gdrive() : from aitextgen.colab import mount_gdrive , copy_file_to_gdrive mount_gdrive () You'll be asked for an auth code; input it and press enter, and a My Drive folder will appear in Colab Files view. You can drag and drop the model files into the Google Drive, or use copy_file_to_gdrive to copy them programmatically. copy_file_to_gdrive ( \"pytorch_model.bin\" ) copy_file_to_gdrive ( \"config.json\" ) Saving During Training \u00b6 By default, the train() function has save_every = 1000 , which means the model will save every 1000 steps to the specified output_dir ( trained_model by default). You can adjust as necessary. Saving During Training in Google Colab \u00b6 Concerned about timeouts in Google Colab? aitextgen has a feature that will copy models to your Google Drive periodically in case the instance gets killed! As long as your drive is mounted as above, pass save_gdrive = True to the train() function: ai . train ( save_gdrive = True ) This will save the model to the folder corresponding to the training run_id parameter (the datetime training was called, to prevent accidently overwriting).","title":"Saving a Model"},{"location":"save-model/#model-saving","text":"There are are multiple ways to save models. Whenever a model is saved, two files are generated: pytorch_model.bin which contains the model weights, and config.json which is needed to load the model if it is not the base 124M GPT-2. Assuming we have an aitextgen model ai :","title":"Model Saving"},{"location":"save-model/#ad-hoc-saving","text":"The aitextgen model can be saved at any time using save . ai . save ()","title":"Ad Hoc saving"},{"location":"save-model/#save-to-google-drive","text":"If you are using Google Colaboratory, you can mount your personal Google Drive to the notebook and save your models there. Downloading models from Colab Notebooks It's strongly recommended to move models to Google Drive before downloading them from Colaboratory. First mount your Google Drive using mount_gdrive() : from aitextgen.colab import mount_gdrive , copy_file_to_gdrive mount_gdrive () You'll be asked for an auth code; input it and press enter, and a My Drive folder will appear in Colab Files view. You can drag and drop the model files into the Google Drive, or use copy_file_to_gdrive to copy them programmatically. copy_file_to_gdrive ( \"pytorch_model.bin\" ) copy_file_to_gdrive ( \"config.json\" )","title":"Save to Google Drive"},{"location":"save-model/#saving-during-training","text":"By default, the train() function has save_every = 1000 , which means the model will save every 1000 steps to the specified output_dir ( trained_model by default). You can adjust as necessary.","title":"Saving During Training"},{"location":"save-model/#saving-during-training-in-google-colab","text":"Concerned about timeouts in Google Colab? aitextgen has a feature that will copy models to your Google Drive periodically in case the instance gets killed! As long as your drive is mounted as above, pass save_gdrive = True to the train() function: ai . train ( save_gdrive = True ) This will save the model to the folder corresponding to the training run_id parameter (the datetime training was called, to prevent accidently overwriting).","title":"Saving During Training in Google Colab"},{"location":"upload/","text":"Upload Model to Huggingface \u00b6 You can upload your trained models to Huggingface, where it can be downloaded by others! To upload your model, you'll have to create a folder which has 6 files: pytorch_model.bin config.json vocab.json merges.txt special_tokens_map.json tokenizer_config.json You can generate all of these files at the same time into a given folder by running ai.save_for_upload(model_name) . Then, follow the transformers-cli instructions to upload the model. transformers-cli login transformers-cli upload model_name You (or another user) can download cache, and generate from that model via: ai = aitextgen(model=\"username/model_name\")","title":"Upload Model to Huggingface"},{"location":"upload/#upload-model-to-huggingface","text":"You can upload your trained models to Huggingface, where it can be downloaded by others! To upload your model, you'll have to create a folder which has 6 files: pytorch_model.bin config.json vocab.json merges.txt special_tokens_map.json tokenizer_config.json You can generate all of these files at the same time into a given folder by running ai.save_for_upload(model_name) . Then, follow the transformers-cli instructions to upload the model. transformers-cli login transformers-cli upload model_name You (or another user) can download cache, and generate from that model via: ai = aitextgen(model=\"username/model_name\")","title":"Upload Model to Huggingface"},{"location":"tutorials/colab/","text":"Colaboratory Notebooks \u00b6 You cannot finetune OpenAI's GPT-2 models on CPU (and not even on some consumer GPUs). Therefore, there are a couple Google Colaboratory notebooks, which provide a GPU suitable for finetuning a model. The Colab Notebooks also contain utilities to make it easier to export the model to Google Drive during and after training. Finetuning OpenAI's Model \u00b6 Colab Notebook A Notebook for finetuning OpenAI's model on a GPU. This is the most common use case. 124M Only Currently you can only finetune the 124M OpenAI GPT-2 model. Training Your Own GPT-2 Model \u00b6 Colab Notebook A Notebook for creating your own GPT-2 model with your own tokenizer. See the Model From Scratch on the advantages and disadvantages of this approach.","title":"Colaboratory Notebooks"},{"location":"tutorials/colab/#colaboratory-notebooks","text":"You cannot finetune OpenAI's GPT-2 models on CPU (and not even on some consumer GPUs). Therefore, there are a couple Google Colaboratory notebooks, which provide a GPU suitable for finetuning a model. The Colab Notebooks also contain utilities to make it easier to export the model to Google Drive during and after training.","title":"Colaboratory Notebooks"},{"location":"tutorials/colab/#finetuning-openais-model","text":"Colab Notebook A Notebook for finetuning OpenAI's model on a GPU. This is the most common use case. 124M Only Currently you can only finetune the 124M OpenAI GPT-2 model.","title":"Finetuning OpenAI's Model"},{"location":"tutorials/colab/#training-your-own-gpt-2-model","text":"Colab Notebook A Notebook for creating your own GPT-2 model with your own tokenizer. See the Model From Scratch on the advantages and disadvantages of this approach.","title":"Training Your Own GPT-2 Model"},{"location":"tutorials/generate_1_5b/","text":"Generating From GPT-2 1.5B \u00b6 Want to generate a ton of text with the largest GPT-2 model, with the generation control provided by aitextgen? Now you can, at a surprisingly low cost! ($0.382/hr, prorated to the nearest second) Here's how to set it up on Google Cloud Platform. Setting Up an AI Platform Notebook \u00b6 An AI Platform Notebook is a hosted Jupyter Lab instance on Google Cloud Platform oriented for AI training and inference. Since it requires zero setup and has no additional costs outside of CPU/GPUs , it's the best tool to play with aitextgen. First, go to AI Platform Notebooks in the GCP console (if you haven't made a project + billing, it should prompt you to do so). Go to New Instance , select PyTorch 1.4 and With 1 NVIDIA Tesla T4 . Quotas You may need T4 quota to create a VM with a T4; accounts should have enough by default, but you may want to confirm. The rest of the VM config settings are fine to leave as/is, however make sure you check Install NVIDIA GPU driver automatically for me ! Once the instance is created, wait a bit (it takes awhile to install the driver), and a OPEN JUPYTERLAB button will appear. Click it to open the hosted Jupyter Lab Installing aitextgen on the VM \u00b6 Now we have to install the dependencies, which only have to be done once. In the Jupyter Lab instance, open a Terminal tab, and install both aitextgen and tensorflow (we'll need tensorflow later) pip3 install aitextgen tensorflow Now the harder part: we need to install and compile apex for FP16 support with the T4 GPU. To do that, run: git clone https://github.com/NVIDIA/apex cd apex && pip install -v --no-cache-dir --global-option = \"--cpp_ext\" --global-option = \"--cuda_ext\" ./ That will take a few minutes, but once that is done, you are good to go and do not need to rerun these steps again! Loading GPT-2 1.5B \u00b6 Now go back to the Launcher and create a Python 3 Notebook (or upload the one here). CUDA You may want to ensure the Notebook sees the CUDA installation, which appears to be somewhat random. This can be verified by running import torch in a cell, then torch.cuda.is_available() . In a cell, load aitextgen: from aitextgen import aitextgen In another cell, input and run: ai = aitextgen ( tf_gpt2 = \"1558M\" , to_gpu = True , to_fp16 = True ) A few things going on here: The TensorFlow-based GPT-2 1.5B is downloaded from Google's servers. (download rate is very fast). This download will only occur once. It is converted to a corresponding PyTorch model, and then loaded. After it is loaded, it is converted to a FP16 representation. Then it is moved to the T4 GPU. Generating from GPT-2 1.5B \u00b6 Now we can generate texts! The T4, for GPT-2 1.5B in FP16 mode, can generate about 30 texts in a batch without going OOM. (you can verify GPU memory usage at any time by opening up a Terminal and running nvidia-smi ) Create a cell and add: ai . generate_to_file ( n = 300 , batch_size = 30 ) Batch Size The batch size of 30 above assumes the default max_length of 256. If you want to use the full 1024 token max length, lower the batch size to 15, as the GPU will go OOM otherwise. And it will generate the texts to a file! When completed, you can double-click to view it in Jupyter Lab, and you can download the file by right-clicking it from the file viewer. More importantly, all parameters to generate are valid, allowing massive flexibility! ai . generate_to_file ( n = 150 , batch_size = 15 , max_length = 1024 , top_p = 0.9 , temperature = 1.2 , prompt = \"President Donald Trump has magically transformed into a unicorn.\" ) Cleanup \u00b6 Make sure you Stop the instance when you are done to avoid being charged . To do that, go back to the AI Platform Notebook console, select the instance, and press Stop. Since 100GB of storage may be pricy, you may want to consider deleting the VM fully if you are done with it as well.","title":"Generating From GPT-2 1.5B"},{"location":"tutorials/generate_1_5b/#generating-from-gpt-2-15b","text":"Want to generate a ton of text with the largest GPT-2 model, with the generation control provided by aitextgen? Now you can, at a surprisingly low cost! ($0.382/hr, prorated to the nearest second) Here's how to set it up on Google Cloud Platform.","title":"Generating From GPT-2 1.5B"},{"location":"tutorials/generate_1_5b/#setting-up-an-ai-platform-notebook","text":"An AI Platform Notebook is a hosted Jupyter Lab instance on Google Cloud Platform oriented for AI training and inference. Since it requires zero setup and has no additional costs outside of CPU/GPUs , it's the best tool to play with aitextgen. First, go to AI Platform Notebooks in the GCP console (if you haven't made a project + billing, it should prompt you to do so). Go to New Instance , select PyTorch 1.4 and With 1 NVIDIA Tesla T4 . Quotas You may need T4 quota to create a VM with a T4; accounts should have enough by default, but you may want to confirm. The rest of the VM config settings are fine to leave as/is, however make sure you check Install NVIDIA GPU driver automatically for me ! Once the instance is created, wait a bit (it takes awhile to install the driver), and a OPEN JUPYTERLAB button will appear. Click it to open the hosted Jupyter Lab","title":"Setting Up an AI Platform Notebook"},{"location":"tutorials/generate_1_5b/#installing-aitextgen-on-the-vm","text":"Now we have to install the dependencies, which only have to be done once. In the Jupyter Lab instance, open a Terminal tab, and install both aitextgen and tensorflow (we'll need tensorflow later) pip3 install aitextgen tensorflow Now the harder part: we need to install and compile apex for FP16 support with the T4 GPU. To do that, run: git clone https://github.com/NVIDIA/apex cd apex && pip install -v --no-cache-dir --global-option = \"--cpp_ext\" --global-option = \"--cuda_ext\" ./ That will take a few minutes, but once that is done, you are good to go and do not need to rerun these steps again!","title":"Installing aitextgen on the VM"},{"location":"tutorials/generate_1_5b/#loading-gpt-2-15b","text":"Now go back to the Launcher and create a Python 3 Notebook (or upload the one here). CUDA You may want to ensure the Notebook sees the CUDA installation, which appears to be somewhat random. This can be verified by running import torch in a cell, then torch.cuda.is_available() . In a cell, load aitextgen: from aitextgen import aitextgen In another cell, input and run: ai = aitextgen ( tf_gpt2 = \"1558M\" , to_gpu = True , to_fp16 = True ) A few things going on here: The TensorFlow-based GPT-2 1.5B is downloaded from Google's servers. (download rate is very fast). This download will only occur once. It is converted to a corresponding PyTorch model, and then loaded. After it is loaded, it is converted to a FP16 representation. Then it is moved to the T4 GPU.","title":"Loading GPT-2 1.5B"},{"location":"tutorials/generate_1_5b/#generating-from-gpt-2-15b_1","text":"Now we can generate texts! The T4, for GPT-2 1.5B in FP16 mode, can generate about 30 texts in a batch without going OOM. (you can verify GPU memory usage at any time by opening up a Terminal and running nvidia-smi ) Create a cell and add: ai . generate_to_file ( n = 300 , batch_size = 30 ) Batch Size The batch size of 30 above assumes the default max_length of 256. If you want to use the full 1024 token max length, lower the batch size to 15, as the GPU will go OOM otherwise. And it will generate the texts to a file! When completed, you can double-click to view it in Jupyter Lab, and you can download the file by right-clicking it from the file viewer. More importantly, all parameters to generate are valid, allowing massive flexibility! ai . generate_to_file ( n = 150 , batch_size = 15 , max_length = 1024 , top_p = 0.9 , temperature = 1.2 , prompt = \"President Donald Trump has magically transformed into a unicorn.\" )","title":"Generating from GPT-2 1.5B"},{"location":"tutorials/generate_1_5b/#cleanup","text":"Make sure you Stop the instance when you are done to avoid being charged . To do that, go back to the AI Platform Notebook console, select the instance, and press Stop. Since 100GB of storage may be pricy, you may want to consider deleting the VM fully if you are done with it as well.","title":"Cleanup"},{"location":"tutorials/hello-world/","text":"Hello World \u00b6 Here's how you can quickly test out aitextgen on your own computer, even if you don't have a GPU! For generating text from a pretrained GPT-2 model: from aitextgen import aitextgen # Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model ai = aitextgen () ai . generate () ai . generate ( n = 3 , max_length = 100 ) ai . generate ( n = 3 , prompt = \"I believe in unicorns because\" , max_length = 100 ) ai . generate_to_file ( n = 10 , prompt = \"I believe in unicorns because\" , max_length = 100 , temperature = 1.2 ) You can also generate from the command line: aitextgen generate aitextgen generate --prompt \"I believe in unicorns because\" --to_file False Want to train your own mini GPT-2 model on your own computer? Download this text file of Shakespeare plays , cd to that directory in a Teriminal, open up a python3 console and go: from aitextgen.TokenDataset import TokenDataset from aitextgen.tokenizers import train_tokenizer from aitextgen.utils import GPT2ConfigCPU from aitextgen import aitextgen # The name of the downloaded Shakespeare text for training file_name = \"input.txt\" # Train a custom BPE Tokenizer on the downloaded text # This will save two files: aitextgen-vocab.json and aitextgen-merges.txt, # which are needed to rebuild the tokenizer. train_tokenizer ( file_name ) vocab_file = \"aitextgen-vocab.json\" merges_file = \"aitextgen-merges.txt\" # GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training # e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2. config = GPT2ConfigCPU () # Instantiate aitextgen using the created tokenizer and config ai = aitextgen ( vocab_file = vocab_file , merges_file = merges_file , config = config ) # You can build datasets for training by creating TokenDatasets, # which automatically processes the dataset with the appropriate size. data = TokenDataset ( file_name , vocab_file = vocab_file , merges_file = merges_file , block_size = 64 ) # Train the model! It will save pytorch_model.bin periodically and after completion. # On a 2016 MacBook Pro, this took ~25 minutes to run. ai . train ( data , batch_size = 16 , num_steps = 5000 ) # Generate text from it! ai . generate ( 10 , prompt = \"ROMEO:\" )","title":"Hello World Tutorial"},{"location":"tutorials/hello-world/#hello-world","text":"Here's how you can quickly test out aitextgen on your own computer, even if you don't have a GPU! For generating text from a pretrained GPT-2 model: from aitextgen import aitextgen # Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model ai = aitextgen () ai . generate () ai . generate ( n = 3 , max_length = 100 ) ai . generate ( n = 3 , prompt = \"I believe in unicorns because\" , max_length = 100 ) ai . generate_to_file ( n = 10 , prompt = \"I believe in unicorns because\" , max_length = 100 , temperature = 1.2 ) You can also generate from the command line: aitextgen generate aitextgen generate --prompt \"I believe in unicorns because\" --to_file False Want to train your own mini GPT-2 model on your own computer? Download this text file of Shakespeare plays , cd to that directory in a Teriminal, open up a python3 console and go: from aitextgen.TokenDataset import TokenDataset from aitextgen.tokenizers import train_tokenizer from aitextgen.utils import GPT2ConfigCPU from aitextgen import aitextgen # The name of the downloaded Shakespeare text for training file_name = \"input.txt\" # Train a custom BPE Tokenizer on the downloaded text # This will save two files: aitextgen-vocab.json and aitextgen-merges.txt, # which are needed to rebuild the tokenizer. train_tokenizer ( file_name ) vocab_file = \"aitextgen-vocab.json\" merges_file = \"aitextgen-merges.txt\" # GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training # e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2. config = GPT2ConfigCPU () # Instantiate aitextgen using the created tokenizer and config ai = aitextgen ( vocab_file = vocab_file , merges_file = merges_file , config = config ) # You can build datasets for training by creating TokenDatasets, # which automatically processes the dataset with the appropriate size. data = TokenDataset ( file_name , vocab_file = vocab_file , merges_file = merges_file , block_size = 64 ) # Train the model! It will save pytorch_model.bin periodically and after completion. # On a 2016 MacBook Pro, this took ~25 minutes to run. ai . train ( data , batch_size = 16 , num_steps = 5000 ) # Generate text from it! ai . generate ( 10 , prompt = \"ROMEO:\" )","title":"Hello World"},{"location":"tutorials/model-from-scratch/","text":"Training a GPT-2 Model From Scratch \u00b6 The original GPT-2 model released by OpenAI was trained on English webpages linked to from Reddit, with a strong bias toward longform content (multiple paragraphs). If that is not your use case, you may get a better generation quality and speed by training your own model and Tokenizer. Examples of good use cases: Short-form content (e.g. Tweets, Reddit post titles) Non-English Text Heavily Encoded Text It still will require a massive amount of training time (several hours, even on a TPU), but will be more flexible. Building a Custom Tokenizer. \u00b6 The train_tokenizer() function from aitextgen.tokenizers trains the model on the specified text(s) on disk. Vocabulary Size The default vocabulary size for train_tokenizer() is 5,000 tokens. Although this is much lower than GPT-2's 50k vocab size, the smaller the vocab size, the easier it is to train the model (since it's more likely for the model to make a correct \"guess\"), and the model file size will be much smaller. from aitextgen.tokenizers import train_tokenizer train_tokenizer ( file_name ) This creates two files: aitextgen-vocab.json and aitextgen-merges.txt , which are needed to rebuild the tokenizer. Building a Custom Dataset \u00b6 You can build a TokenDataset based off your custom Tokenizer, to be fed into the model. data = TokenDataset ( file_name , vocab_file = vocab_file , merges_file = merges_file , block_size = 32 ) Building a Custom Config \u00b6 Whenever you load a default 124M GPT-2 model, it uses a GPT2Config() under the hood. But you can create your own, with whatever parameters you want. The build_gpt2_config() function from aitextgen.utils gives you more control. config = build_gpt2_config ( vocab_size = 5000 , max_length = 32 , dropout = 0.0 , n_embd = 256 , n_layer = 8 , n_head = 8 ) A few notes on the inputs: vocab_size : Vocabulary size: this must match what you used to build the tokenizer! max_length : Context window for the GPT-2 model: this must match the block_size used in the TokenDataset! dropout : Dropout on various areas of the model to limit overfitting (you should likely keep at 0) n_embd : The embedding size for each vocab token. n_layers : Transformer layers n_head : Transformer heads Model Size GPT-2 Model size is directly proportional to vocab_size * embeddings . Training the Custom Model \u00b6 You can instantiate an empty GPT-2 according to your custom config, and construct a custom tokenizer according to your vocab and merges file: ai = aitextgen ( vocab_file = vocab_file , merges_file = merges_file , config = config ) Training is done as normal. ai . train ( data , batch_size = 16 , num_steps = 5000 ) Reloading the Custom Model \u00b6 You'll always need to provide the vocab_file, merges_file, and config (a config file is saved when the model is saved; you can either build it at runtime as above, or use the config.json ) ai = aitextgen ( model = \"pytorch_model.bin\" , vocab_file = vocab_file , merges_file = merges_file , config = config )","title":"Training a GPT-2 Model From Scratch"},{"location":"tutorials/model-from-scratch/#training-a-gpt-2-model-from-scratch","text":"The original GPT-2 model released by OpenAI was trained on English webpages linked to from Reddit, with a strong bias toward longform content (multiple paragraphs). If that is not your use case, you may get a better generation quality and speed by training your own model and Tokenizer. Examples of good use cases: Short-form content (e.g. Tweets, Reddit post titles) Non-English Text Heavily Encoded Text It still will require a massive amount of training time (several hours, even on a TPU), but will be more flexible.","title":"Training a GPT-2 Model From Scratch"},{"location":"tutorials/model-from-scratch/#building-a-custom-tokenizer","text":"The train_tokenizer() function from aitextgen.tokenizers trains the model on the specified text(s) on disk. Vocabulary Size The default vocabulary size for train_tokenizer() is 5,000 tokens. Although this is much lower than GPT-2's 50k vocab size, the smaller the vocab size, the easier it is to train the model (since it's more likely for the model to make a correct \"guess\"), and the model file size will be much smaller. from aitextgen.tokenizers import train_tokenizer train_tokenizer ( file_name ) This creates two files: aitextgen-vocab.json and aitextgen-merges.txt , which are needed to rebuild the tokenizer.","title":"Building a Custom Tokenizer."},{"location":"tutorials/model-from-scratch/#building-a-custom-dataset","text":"You can build a TokenDataset based off your custom Tokenizer, to be fed into the model. data = TokenDataset ( file_name , vocab_file = vocab_file , merges_file = merges_file , block_size = 32 )","title":"Building a Custom Dataset"},{"location":"tutorials/model-from-scratch/#building-a-custom-config","text":"Whenever you load a default 124M GPT-2 model, it uses a GPT2Config() under the hood. But you can create your own, with whatever parameters you want. The build_gpt2_config() function from aitextgen.utils gives you more control. config = build_gpt2_config ( vocab_size = 5000 , max_length = 32 , dropout = 0.0 , n_embd = 256 , n_layer = 8 , n_head = 8 ) A few notes on the inputs: vocab_size : Vocabulary size: this must match what you used to build the tokenizer! max_length : Context window for the GPT-2 model: this must match the block_size used in the TokenDataset! dropout : Dropout on various areas of the model to limit overfitting (you should likely keep at 0) n_embd : The embedding size for each vocab token. n_layers : Transformer layers n_head : Transformer heads Model Size GPT-2 Model size is directly proportional to vocab_size * embeddings .","title":"Building a Custom Config"},{"location":"tutorials/model-from-scratch/#training-the-custom-model","text":"You can instantiate an empty GPT-2 according to your custom config, and construct a custom tokenizer according to your vocab and merges file: ai = aitextgen ( vocab_file = vocab_file , merges_file = merges_file , config = config ) Training is done as normal. ai . train ( data , batch_size = 16 , num_steps = 5000 )","title":"Training the Custom Model"},{"location":"tutorials/model-from-scratch/#reloading-the-custom-model","text":"You'll always need to provide the vocab_file, merges_file, and config (a config file is saved when the model is saved; you can either build it at runtime as above, or use the config.json ) ai = aitextgen ( model = \"pytorch_model.bin\" , vocab_file = vocab_file , merges_file = merges_file , config = config )","title":"Reloading the Custom Model"}]}